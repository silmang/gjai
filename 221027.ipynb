{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in .\\venv\\lib\\site-packages (4.23.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in .\\venv\\lib\\site-packages (from transformers) (0.10.1)\n",
      "Requirement already satisfied: importlib-metadata in .\\venv\\lib\\site-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in .\\venv\\lib\\site-packages (from transformers) (0.13.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in .\\venv\\lib\\site-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: packaging>=20.0 in .\\venv\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: filelock in .\\venv\\lib\\site-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\venv\\lib\\site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in .\\venv\\lib\\site-packages (from transformers) (2022.9.13)\n",
      "Requirement already satisfied: requests in .\\venv\\lib\\site-packages (from transformers) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in .\\venv\\lib\\site-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .\\venv\\lib\\site-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.3.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in .\\venv\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in .\\venv\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.5)\n",
      "Requirement already satisfied: zipp>=0.5 in .\\venv\\lib\\site-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in .\\venv\\lib\\site-packages (from requests->transformers) (2.1.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\venv\\lib\\site-packages (from requests->transformers) (2022.9.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\venv\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in .\\venv\\lib\\site-packages (from requests->transformers) (1.26.12)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sacremoses in .\\venv\\lib\\site-packages (0.0.53)\n",
      "Requirement already satisfied: regex in .\\venv\\lib\\site-packages (from sacremoses) (2022.9.13)\n",
      "Requirement already satisfied: click in .\\venv\\lib\\site-packages (from sacremoses) (8.1.3)\n",
      "Requirement already satisfied: six in .\\venv\\lib\\site-packages (from sacremoses) (1.16.0)\n",
      "Requirement already satisfied: joblib in .\\venv\\lib\\site-packages (from sacremoses) (1.2.0)\n",
      "Requirement already satisfied: tqdm in .\\venv\\lib\\site-packages (from sacremoses) (4.64.1)\n",
      "Requirement already satisfied: importlib-metadata in .\\venv\\lib\\site-packages (from click->sacremoses) (4.12.0)\n",
      "Requirement already satisfied: colorama in .\\venv\\lib\\site-packages (from click->sacremoses) (0.4.5)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in .\\venv\\lib\\site-packages (from importlib-metadata->click->sacremoses) (4.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in .\\venv\\lib\\site-packages (from importlib-metadata->click->sacremoses) (3.8.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\projects\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import TFBertModel, BertTokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, string):\n",
    "    plt.plot(history.history[string])\n",
    "    plt.plot(history.history['val_'+string], '')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(string)\n",
    "    plt.legend([string, 'val_'+string])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(1111)\n",
    "np.random.seed(1111)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "CLASS_NUMBER = 2\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 2\n",
    "VALID_SPLIT = 0.2\n",
    "MAX_LEN = 40\n",
    "BERT_CKPT = 'c:\\\\pytest\\\\data\\\\KOR\\\\BERT\\\\bert_ckpt\\\\'\n",
    "DATA_IN_PATH = 'c:\\\\pytest\\\\data\\\\KOR\\\\naver_movie\\\\data_in\\\\'\n",
    "DATA_OUT_PATH = \"c:\\\\pytest\\\\data\\\\KOR\\\\BERT\\\\data_out\\\\\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listToString(listdata):\n",
    "    result = 'id\\tdocument\\tlabel\\n'\n",
    "    for data_each in listdata:\n",
    "        if data_each:\n",
    "            result += data_each[0]+\"\\t\"+data_each[1]+\"\\t\"+data_each[2]+\"\\n\"\n",
    "    return result\n",
    "\n",
    "\n",
    "def read_data(filename, encoding='cp949', start=0):\n",
    "    with open(filename, 'r', encoding=encoding) as f:\n",
    "        data = [line.split('\\t') for line in f.read().splitlines()]\n",
    "        data = data[start:]\n",
    "    return data\n",
    "\n",
    "\n",
    "def write_data(data, filename, encoding='cp949'):\n",
    "    with open(filename, 'w', encoding=encoding) as f:\n",
    "        f.write(data)\n",
    "\n",
    "\n",
    "data_ratings = read_data(os.path.join(DATA_IN_PATH, \"ratings_utf8_small.txt\"), encoding='utf-8', start=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ratings_train, ratings_test = train_test_split(data_ratings)\n",
    "\n",
    "ratings_train = listToString(ratings_train)\n",
    "ratings_test = listToString(ratings_test)\n",
    "\n",
    "write_data(ratings_train, os.path.join(DATA_IN_PATH, \"ratings_train.txt\"), encoding='utf-8')\n",
    "write_data(ratings_test, os.path.join(DATA_IN_PATH, \"ratings_test.txt\"), encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 996k/996k [00:03<00:00, 274kB/s]  \n",
      "c:\\projects\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\pytest\\data\\KOR\\BERT\\bert_ckpt\\tokenizer. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 14.6kB/s]\n",
      "Downloading: 100%|██████████| 625/625 [00:00<00:00, 628kB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\", cache_dir=os.path.join(BERT_CKPT, \"tokenizer\"), do_lower_case=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\pytest\\data\\KOR\\BERT\\data_out\\ -- Folder already exists\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "if os.path.exists(DATA_OUT_PATH):\n",
    "    print(\"{} -- Folder already exists\\n\".format(DATA_OUT_PATH))\n",
    "else:\n",
    "    os.makedirs(DATA_OUT_PATH, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete\\n\".format(DATA_OUT_PATH))\n",
    "with open(DATA_OUT_PATH+\"bert_tokenizer.pickle\", 'wb') as file:\n",
    "    pickle.dump(tokenizer, file, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 119, 102]\n",
      "['[ C L S ]', '안', '# # 녕', '# # 하', '# # 세', '# # 요', ',', '반', '# # 갑', '# # 습', '# # 니 다', '.', '[ S E P ]']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"안녕하세요, 반갑습니다.\"\n",
    "encode = tokenizer.encode(test_sentence)\n",
    "token_print = [tokenizer.decode(token) for token in encode]\n",
    "encode = tokenizer.encode(test_sentence)\n",
    "print(encode)\n",
    "print(token_print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>document</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7743368</td>\n",
       "      <td>박흥식영화 사랑해 말순씨 ***보면 장애우를 배려하지않는 저질 영화장애우가 성희롱이...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10067386</td>\n",
       "      <td>안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4254270</td>\n",
       "      <td>성형부작용 같은 주인공 얼굴 때문에 집중이 안됨..;;;</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>530171</td>\n",
       "      <td>맥티어난의 최고 졸작품</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9462634</td>\n",
       "      <td>어린 외계인역 하신분 귀엽네요..ㅎ</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                           document  label\n",
       "0   7743368  박흥식영화 사랑해 말순씨 ***보면 장애우를 배려하지않는 저질 영화장애우가 성희롱이...      0\n",
       "1  10067386                        안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화.      1\n",
       "2   4254270                    성형부작용 같은 주인공 얼굴 때문에 집중이 안됨..;;;      0\n",
       "3    530171                                       맥티어난의 최고 졸작품      0\n",
       "4   9462634                                어린 외계인역 하신분 귀엽네요..ㅎ      1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATA_TRAIN_PATH = os.path.join(DATA_IN_PATH, \"ratings_train.txt\")\n",
    "DATA_TEST_PATH = os.path.join(DATA_IN_PATH, \"ratings_test.txt\")\n",
    "\n",
    "train_data = pd.read_csv(DATA_TRAIN_PATH, header=0, delimiter='\\t', quoting=3)\n",
    "train_data = train_data.dropna()\n",
    "train_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 102]\n",
      "['[ C L S ]', '안', '# # 녕', '# # 하', '# # 세', '# # 요', ',', '반', '# # 갑', '# # 습', '# # 니 다', '[ S E P ]']\n",
      "[101, 31178, 11356, 102]\n",
      "['[ C L S ]', 'H e l l o', 'w o r l d', '[ S E P ]']\n",
      "[CLS] 안녕하세요, 반갑습니다 [SEP]\n",
      "[CLS] Hello world [SEP]\n"
     ]
    }
   ],
   "source": [
    "kor_encode = tokenizer.encode(\"안녕하세요, 반갑습니다\")\n",
    "eng_encode = tokenizer.encode(\"Hello world\")\n",
    "kor_decode = tokenizer.decode(kor_encode)\n",
    "eng_decode = tokenizer.decode(eng_encode)\n",
    "print(kor_encode)\n",
    "print([tokenizer.decode(token) for token in kor_encode])\n",
    "print(eng_encode)\n",
    "print([tokenizer.decode(token) for token in eng_encode])\n",
    "print(kor_decode)\n",
    "print(eng_decode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[UNK]', '[SEP]', '[PAD]', '[CLS]', '[MASK]'] \n",
      " [100, 102, 0, 101, 103]\n",
      "[101, 9521, 118741, 35506, 24982, 48549, 117, 9321, 118610, 119081, 48345, 119, 102]\n",
      "['[ C L S ]', '안', '# # 녕', '# # 하', '# # 세', '# # 요', ',', '반', '# # 갑', '# # 습', '# # 니 다', '.', '[ S E P ]']\n",
      "[101, 31178, 11356, 102]\n",
      "['[ C L S ]', 'H e l l o', 'w o r l d', '[ S E P ]']\n",
      "[CLS] 안녕하세요, 반갑습니다. [SEP]\n",
      "[CLS] Hello world [SEP]\n"
     ]
    }
   ],
   "source": [
    "# 16\n",
    "print(tokenizer.all_special_tokens, \"\\n\", tokenizer.all_special_ids)\n",
    "kor_encode = tokenizer.encode(\"안녕하세요, 반갑습니다.\")\n",
    "eng_encode = tokenizer.encode(\"Hello world\")\n",
    "kor_decode = tokenizer.decode(kor_encode)\n",
    "eng_decode = tokenizer.decode(eng_encode)\n",
    "print(kor_encode)\n",
    "print([tokenizer.decode(token) for token in kor_encode])\n",
    "print(eng_encode)\n",
    "print([tokenizer.decode(token) for token in eng_encode])\n",
    "print(kor_decode)\n",
    "print(eng_decode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_tokenizer(sent, MAX_LEN):\n",
    "    encoded_dict = tokenizer.encode_plus(text=sent, add_special_tokens=True, max_length=MAX_LEN,\n",
    "                                         padding='max_length', truncation=True, return_attention_mask=True) #padding적용에사용할문장의최대길이(‘longest’, ‘max_length’, ‘do_not_pad’)\n",
    "    input_id = encoded_dict['input_ids']\n",
    "    attention_mask = encoded_dict['attention_mask']\n",
    "    token_type_id = encoded_dict['token_type_ids']\n",
    "    return input_id, attention_mask, token_type_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:00<00:00, 3269.17it/s]\n"
     ]
    }
   ],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "train_data_labels = []\n",
    "for train_sent, train_label in tqdm(zip(train_data[\"document\"], train_data[\"label\"]), total=len(train_data)):\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(train_sent, MAX_LEN)\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        train_data_labels.append(train_label)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(train_sent)\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sents:375, # labels:375\n"
     ]
    }
   ],
   "source": [
    "train_movie_input_ids = np.array(input_ids, dtype=int)\n",
    "train_movie_attention_masks = np.array(attention_masks, dtype=int)\n",
    "train_movie_type_ids = np.array(token_type_ids, dtype=int)\n",
    "train_movie_inputs = (train_movie_input_ids, train_movie_attention_masks, train_movie_type_ids)\n",
    "train_data_labels = np.asarray(train_data_labels, dtype=np.int32)\n",
    "print(\"# sents:{}, # labels:{}\".format(len(train_movie_input_ids), len(train_data_labels)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   101   9521  21789   9651 119168  11102   9326  35506 118762  10530\n",
      "   9138  13767   9757  48210  89851  18589  42428    119    102      0\n",
      "      0      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n",
      "[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0]\n",
      "[CLS] 안개 자욱한 밤하늘에 떠 있는 초승달 같은 영화. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n"
     ]
    }
   ],
   "source": [
    "input_id = train_movie_input_ids[1]\n",
    "attention_mask = train_movie_attention_masks[1]\n",
    "token_type_id = train_movie_type_ids[1]\n",
    "\n",
    "print(input_id)\n",
    "print(attention_mask)\n",
    "print(token_type_id)\n",
    "print(tokenizer.decode(input_id))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFBertClassifier(tf.keras.Model):\n",
    "    def __init__(self, model_name, dir_path, num_class):\n",
    "        super().__init__()\n",
    "        self.bert = TFBertModel.from_pretrained(model_name, cache_dir=dir_path)\n",
    "        self.dropout = tf.keras.layers.Dropout(self.bert.config.hidden_dropout_prob)\n",
    "        self.classifier = tf.keras.layers.Dense(num_class, name=\"classifier\", activation=\"softmax\",\n",
    "                                                kernel_initializer=tf.keras.initializers.TruncatedNormal(self.bert.config.initializer_range))\n",
    "\n",
    "    def call(self, inputs, attention_mask=None, token_type_ids=None, training=False):\n",
    "        outputs = self.bert(inputs, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
    "        pooler_output = outputs[1]\n",
    "        pooler_output = self.dropout(pooler_output, training=training)\n",
    "        logits = self.classifier(pooler_output)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_model = TFBertClassifier(model_name='bert-base-multilingual-cased',\n",
    "                             dir_path=os.path.join(BERT_CKPT, \"model\"), num_class=CLASS_NUMBER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(3e-5)\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "cls_model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"tf2_bert\"\n",
    "earlystop_callback = EarlyStopping(monitor='val_accuracy', min_delta=0.0001, patience=2)\n",
    "checkpoint_path = os.path.join(DATA_OUT_PATH, model_name, 'weights.h5')\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "if os.path.exists(checkpoint_dir):\n",
    "    print(\"{} -- Folder already exists\\n\".format(checkpoint_dir))\n",
    "else:\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    print(\"{} -- Folder create complete\\n\".format(checkpoint_dir))\n",
    "cp_callback = ModelCheckpoint(checkpoint_path, monitor='val_accuracy',\n",
    "                              verbose=1, save_best_only=True, save_weights_only=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = cls_model.fit(train_movie_inputs, train_data_labels, epochs=NUM_EPOCHS, batch_size=BATCH_SIZE, validation_split=VALID_SPLIT, callbacks=[earlystop_callback, cp_callback])\n",
    "print(history.history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history,'accuracy')\n",
    "plot_graphs(history,'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv(DATA_TEST_PATH,header=0,delimiter='\\t',quoting=3)\n",
    "test_data=test_data.dropna()\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_movie_inputs=(test_movie_input_ids,test_movie_attention_masks,test_movie_type_ids)\n",
    "test_data_labels=np.asarray(test_data_labels,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = []\n",
    "attention_masks = []\n",
    "token_type_ids = []\n",
    "test_data_labels = []\n",
    "for test_sent, test_label in tqdm(zip(test_data[\"document\"], test_data[\"label\"])):\n",
    "    try:\n",
    "        input_id, attention_mask, token_type_id = bert_tokenizer(test_sent, MAX_LEN)\n",
    "        input_ids.append(input_id)\n",
    "        attention_masks.append(attention_mask)\n",
    "        token_type_ids.append(token_type_id)\n",
    "        test_data_labels.append(test_label)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "test_movie_input_ids=np.array(input_ids,dtype=int)\n",
    "test_movie_attention_masks=np.array(attention_masks,dtype=int)\n",
    "test_movie_type_ids=np.array(token_type_ids,dtype=int)\n",
    "test_movie_inputs=(test_movie_input_ids,test_movie_attention_masks,test_movie_type_ids)\n",
    "\n",
    "test_data_labels=np.asarray(test_data_labels,dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results=cls_model.evaluate(test_movie_inputs,test_data_labels,batch_size=BATCH_SIZE)\n",
    "print(\"testloss, testacc:\",results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee3a569b6c17f781d88eed465009cab3acc13ebf6747bed2a2b2f4afb9b22959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
