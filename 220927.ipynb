{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU를 사용하고 있습니다.\n",
      "정상이라면 다음과 같이 출력합니다.\n",
      "--->Default GPU Device: /device:GPU:0\n",
      "Default GPU Device:/device:GPU:0\n",
      "Python Version: 3.7.4 (default, Aug  9 2019, 18:34:13) [MSC v.1915 64 bit (AMD64)]\n",
      "Tensorflow Version: 2.10.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import sys\n",
    "if tf.test.gpu_device_name():\n",
    "    print(\"GPU를 사용하고 있습니다.\")\n",
    "    print(\"정상이라면 다음과 같이 출력합니다.\\n--->Default GPU Device: /device:GPU:0\")\n",
    "    print('Default GPU Device:{}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"GPU를 사용하고 있지 않습니다!!!\")\n",
    "    print(\"Please install GPU version of TF\")\n",
    "print(\"Python Version:\", sys.version)\n",
    "print(\"Tensorflow Version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_built_with_cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# perceptron\n",
    "def node(w, x, b):\n",
    "    y = np.sum(w*x)+b\n",
    "    return y\n",
    "\n",
    "def activation(y):\n",
    "    if y>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# and\n",
    "def AND(x1, x2):\n",
    "    x=np.array([x1,x2])\n",
    "    w=np.array([0.5,0.5])\n",
    "    b=-0.7\n",
    "    return activation(node(w,x,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# perceptron\n",
    "def node(w, x, b):\n",
    "    y = np.sum(w*x)+b\n",
    "    return y\n",
    "\n",
    "def activation(y):\n",
    "    if y<0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# nand\n",
    "def NAND(x1, x2):\n",
    "    x=np.array([x1,x2])\n",
    "    w=np.array([0.5,0.5])\n",
    "    b=-0.7\n",
    "    return activation(node(w,x,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# perceptron\n",
    "def node(w, x, b):\n",
    "    y = np.sum(w*x)+b\n",
    "    return y\n",
    "\n",
    "def activation(y):\n",
    "    if y>0:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "        \n",
    "# or\n",
    "def OR(x1, x2):\n",
    "    x=np.array([x1,x2])\n",
    "    w=np.array([0.5,0.5])\n",
    "    b=-0.2\n",
    "    return activation(node(w,x,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "OR(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def XOR(x1, x2):\n",
    "    s1=NAND(x1, x2)\n",
    "    s2=OR(x1,x2)\n",
    "    y=AND(s1,s2)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(XOR(0,0))\n",
    "print(XOR(1,0))\n",
    "print(XOR(0,1))\n",
    "print(XOR(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존 peceptron에 연달아 다른 perceptron을 연결시킨다는 말. 이렇게 되면 더욱 복잡한 연산이 되며 2차원 그래프로 표현했을 경우 어떠한 분류도 수행해 낼 수 있다. 층을 쌓는다는 말은 은닉층을 늘린다는 말과 동일한 표현인데, 이론적으로 한개의 은닉층을 쌓고 적절한 노드를 제공해 준다면 어떠한 경계표현도 할 수 있다는 정리가 있다. 이걸 universal approximation theorem이라고 하며 시벤코 정리라고도 한다. 증명은 복잡하고 그걸 개략적으로 이해할 방법은 여백이 부족해 쓰지 않겠다. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ee3a569b6c17f781d88eed465009cab3acc13ebf6747bed2a2b2f4afb9b22959"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
