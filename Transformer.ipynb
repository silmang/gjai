{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"I_pU20Osdisx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666659787934,"user_tz":-540,"elapsed":2646,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"0df683cb-0081-4838-88fe-ce823466219e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"source":["# 최석재 lingua@naver.com\n","# 구글 드라이브와 연결\n","# from google.colab import auth\n","# auth.authenticate_user()\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","source":["# 경로 설정\n","path = '/content/gdrive/MyDrive/pytest/'\n","!ls '/content/gdrive/MyDrive/pytest/'\n","DATA_OUT_PATH = path+'data/'\n","model_name = 'transformer'"],"metadata":{"id":"KmngkudJd-QB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666659788582,"user_tz":-540,"elapsed":651,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"12ce4377-ed5e-418c-e0c6-9d61df665086"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["'충북 관광 데이터'     iris.csv\t\t\t   뉴스데이터_test.csv\n","'불법 대부업 데이터'   negative.txt\t\t   text_binary_model.h5\n"," 감성대화말뭉치        newfile2.txt\t\t   text_binary_tokenizer.pickle\n"," alice.png\t       newfile.csv\t\t   뉴스데이터_train.csv\n"," base\t\t       pima-indians-diabetes.csv   김소월시.txt\n"," BostonHousing.csv     positive.txt\t\t   윤동주시.txt\n"," chatdata.csv\t       ratings_morphed.txt\t   금융규제운영규정.txt\n"," chatdata_small.csv    ratings_small.txt\t   vocabulary.txt\n"," data\t\t       ratings.txt\t\t   wiki_test.txt\n"," income.csv\t       sonar.csv\t\t   wine.csv\n"," income_test.csv       temp\n"]}]},{"cell_type":"code","source":["# 데이터 확인\n","# 빠른 진행을 위해 small 데이터로 수행한다.\n","# header는 제외하고 로딩\n","import pandas as pd\n","data = pd.read_csv(path+'chatdata_small.csv', names=['Q', 'A', \"label\"], sep=',', header=0, encoding='cp949')\n","print('data length: ', len(data))\n","print('data sample: ', data.head())"],"metadata":{"id":"aH6JIxzDeA6C","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666659788582,"user_tz":-540,"elapsed":23,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"3b7c2660-1c97-466c-9c91-6b4654b0dfb6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data length:  19\n","data sample:                   Q            A  label\n","0           12시 땡!   하루가 또 가네요.      0\n","1      1지망 학교 떨어졌어    위로해 드립니다.      0\n","2     3박4일 놀러가고 싶다  여행은 언제나 좋죠.      0\n","3  3박4일 정도 놀러가고 싶다  여행은 언제나 좋죠.      0\n","4          PPL 심하네   눈살이 찌푸려지죠.      0\n"]}]},{"cell_type":"code","source":["inputs, outputs = list(data['Q']), list(data['A'])\n","print(\"inputs: \", inputs)\n","print(\"outputs: \", outputs)"],"metadata":{"id":"eRI1FncriHbU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666659788582,"user_tz":-540,"elapsed":20,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"5f406e57-db54-4fd6-9193-45c3214adaf6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["inputs:  ['12시 땡!', '1지망 학교 떨어졌어', '3박4일 놀러가고 싶다', '3박4일 정도 놀러가고 싶다', 'PPL 심하네', 'SD카드 망가졌어', 'SD카드 안돼', 'SNS 맞팔 왜 안하지ㅠㅠ', 'SNS 시간낭비인 거 아는데 매일 하는 중', 'SNS 시간낭비인데 자꾸 보게됨', 'SNS보면 나만 빼고 다 행복해보여', '가끔 궁금해', '가끔 뭐하는지 궁금해', '가끔은 혼자인게 좋다', '가난한 자의 설움', '가만 있어도 땀난다', '가상화폐 쫄딱 망함', '가스불 켜고 나갔어', '가스불 켜놓고 나온거 같아']\n","outputs:  ['하루가 또 가네요.', '위로해 드립니다.', '여행은 언제나 좋죠.', '여행은 언제나 좋죠.', '눈살이 찌푸려지죠.', '다시 새로 사는 게 마음 편해요.', '다시 새로 사는 게 마음 편해요.', '잘 모르고 있을 수도 있어요.', '시간을 정하고 해보세요.', '시간을 정하고 해보세요.', '자랑하는 자리니까요.', '그 사람도 그럴 거예요.', '그 사람도 그럴 거예요.', '혼자를 즐기세요.', '돈은 다시 들어올 거예요.', '땀을 식혀주세요.', '어서 잊고 새출발 하세요.', '빨리 집에 돌아가서 끄고 나오세요.', '빨리 집에 돌아가서 끄고 나오세요.']\n"]}]},{"cell_type":"code","source":["# 시작부호와 종료부호 부착\n","# 데이터가 모두 3종이 필요하다. source 언어에는 encoder_input 1개, target 언어에는 decoder_input, decoder_target 2개\n","# encoder는 source 언어를 그대로 사용하면 되나, decoder는 seq2seq의 사용을 위해 <sos>, <eos>를 부착해야 한다\n","# decoder_input 데이터의 시작에는 <sos>, 문장의 끝에는 <eos>를 부착한다\n","# decoder_target 데이터는 <eos>만 필요하다\n","# 어절분리가 되도록 <sos> 뒤에 공백, <eos> 앞에 공백을 두어야 한다\n","outputs_input = data.A.apply(lambda x : '<SOS> '+x+' <EOS>')\n","outputs_target = data.A.apply(lambda x : x+' <EOS>')\n","print('\\noutputs_input:\\n', outputs_input.sample(5))\n","print(\"\\noutputs_target\\n: \", outputs_target.sample(5))"],"metadata":{"id":"5WNwF0YcsIwh","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666659788582,"user_tz":-540,"elapsed":18,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"8febd6c0-5ee8-4336-df10-bc79837a18d1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","outputs_input:\n"," 3            <SOS> 여행은 언제나 좋죠. <EOS>\n","6     <SOS> 다시 새로 사는 게 마음 편해요. <EOS>\n","10           <SOS> 자랑하는 자리니까요. <EOS>\n","16        <SOS> 어서 잊고 새출발 하세요. <EOS>\n","14        <SOS> 돈은 다시 들어올 거예요. <EOS>\n","Name: A, dtype: object\n","\n","outputs_target\n",":  13         혼자를 즐기세요. <EOS>\n","16    어서 잊고 새출발 하세요. <EOS>\n","14    돈은 다시 들어올 거예요. <EOS>\n","3        여행은 언제나 좋죠. <EOS>\n","2        여행은 언제나 좋죠. <EOS>\n","Name: A, dtype: object\n"]}]},{"cell_type":"code","source":["# Data Tokenizing\n","# 각 단어 종류에 대하여 숫자값을 배당한다\n","# 같은 언어 사이에서의 번역이므로, 어휘 목록을 구성하는 토크나이저는 하나만 필요하다\n","# 따라서 input과 output을 결합한다\n","from keras.preprocessing.text import Tokenizer\n","inputs_series = pd.Series(inputs)                                    # outputs_input 과 같은 Series로 변환한다\n","inputs_outputs = pd.concat([inputs_series, outputs_input], axis=0)   # input과 output을 결합한다\n","\n","tokenizer = Tokenizer(num_words=None, char_level=False, lower=False)     # 고빈도 어휘만 사용하려면 num_words에 값을 줄 수 있다. char_level은 False로 해야 한다\n","tokenizer.fit_on_texts(inputs_outputs)     \t                             # inputs와 outputs이 결합된 내용으로 인덱스를 구축한다\n","word_index = tokenizer.word_index                                        # 단어와 인덱스의 쌍을 가져온다\n","\n","print('\\n전체에서 %s개의 고유한 토큰을 찾았습니다.' % len(word_index))\n","print('word_index: ', word_index)\n","print('vocab_size: ', len(word_index))"],"metadata":{"id":"lGdZqOsbq0ac","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666659788582,"user_tz":-540,"elapsed":16,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"ba953327-444a-4a4d-bb8d-43b732a45b87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","전체에서 100개의 고유한 토큰을 찾았습니다.\n","word_index:  {'SOS': 1, 'EOS': 2, 'SNS': 3, '다시': 4, '거예요': 5, '3박4일': 6, '놀러가고': 7, '싶다': 8, 'SD카드': 9, '가끔': 10, '궁금해': 11, '가스불': 12, '여행은': 13, '언제나': 14, '좋죠': 15, '새로': 16, '사는': 17, '게': 18, '마음': 19, '편해요': 20, '시간을': 21, '정하고': 22, '해보세요': 23, '그': 24, '사람도': 25, '그럴': 26, '빨리': 27, '집에': 28, '돌아가서': 29, '끄고': 30, '나오세요': 31, '12시': 32, '땡': 33, '1지망': 34, '학교': 35, '떨어졌어': 36, '정도': 37, 'PPL': 38, '심하네': 39, '망가졌어': 40, '안돼': 41, '맞팔': 42, '왜': 43, '안하지ㅠㅠ': 44, '시간낭비인': 45, '거': 46, '아는데': 47, '매일': 48, '하는': 49, '중': 50, '시간낭비인데': 51, '자꾸': 52, '보게됨': 53, 'SNS보면': 54, '나만': 55, '빼고': 56, '다': 57, '행복해보여': 58, '뭐하는지': 59, '가끔은': 60, '혼자인게': 61, '좋다': 62, '가난한': 63, '자의': 64, '설움': 65, '가만': 66, '있어도': 67, '땀난다': 68, '가상화폐': 69, '쫄딱': 70, '망함': 71, '켜고': 72, '나갔어': 73, '켜놓고': 74, '나온거': 75, '같아': 76, '하루가': 77, '또': 78, '가네요': 79, '위로해': 80, '드립니다': 81, '눈살이': 82, '찌푸려지죠': 83, '잘': 84, '모르고': 85, '있을': 86, '수도': 87, '있어요': 88, '자랑하는': 89, '자리니까요': 90, '혼자를': 91, '즐기세요': 92, '돈은': 93, '들어올': 94, '땀을': 95, '식혀주세요': 96, '어서': 97, '잊고': 98, '새출발': 99, '하세요': 100}\n","vocab_size:  100\n"]}]},{"cell_type":"code","source":["# 토크나이저 저장\n","import os\n","import pickle\n","\n","# DATA_OUT_PATH 경로 생성\n","if os.path.exists(DATA_OUT_PATH + model_name):\n","    print(\"{} -- Folder already exists \\n\".format(DATA_OUT_PATH))\n","else:\n","    os.makedirs(DATA_OUT_PATH + model_name, exist_ok=True)\n","    print(\"{} -- Folder create complete \\n\".format(DATA_OUT_PATH))\n","\n","with open(DATA_OUT_PATH + model_name +\"/transformer.pickle\", \"wb\") as file:\n","  pickle.dump(tokenizer, file, protocol=pickle.HIGHEST_PROTOCOL)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zMu-ApsFXxux","executionInfo":{"status":"ok","timestamp":1666659788583,"user_tz":-540,"elapsed":15,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"bde01ea5-fb2e-4829-9c5c-dbc2babea96c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/MyDrive/pytest/data/ -- Folder already exists \n","\n"]}]},{"cell_type":"code","source":["# Data Sequencing\n","# 배당된 숫자를 이용하여 각 문장의 문자를 숫자로 치환한다\n","# source 언어 Sequencing\n","encoder_input = tokenizer.texts_to_sequences(list(inputs))\n","\n","print('\\nResult of encoder_input sequencing: ')\n","print(inputs[0], encoder_input[0])\n","print(inputs[1], encoder_input[1])\n","print(inputs[2], encoder_input[2])"],"metadata":{"id":"f1Sse1L4HFuA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666659788583,"user_tz":-540,"elapsed":14,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"e50db5e3-aa3f-4186-f55d-6a07159bb8de"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Result of encoder_input sequencing: \n","12시 땡! [32, 33]\n","1지망 학교 떨어졌어 [34, 35, 36]\n","3박4일 놀러가고 싶다 [6, 7, 8]\n"]}]},{"cell_type":"code","source":["# target 언어 Sequencing\n","decoder_input = tokenizer.texts_to_sequences(list(outputs_input))\n","decoder_target = tokenizer.texts_to_sequences(list(outputs_target))\n","\n","print('\\nResult of decoder_input sequencing: ')\n","print(outputs_input[0], decoder_input[0])\n","print(outputs_input[1], decoder_input[1])\n","print(outputs_input[2], decoder_input[2])\n","\n","print('\\nResult of decoder_target sequencing: ')\n","print(outputs_target[0], decoder_target[0])\n","print(outputs_target[1], decoder_target[1])\n","print(outputs_target[2], decoder_target[2])"],"metadata":{"id":"SCDsTkUkRQtF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666659788583,"user_tz":-540,"elapsed":13,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"5e035029-a04e-495b-c420-ad79e30e9963"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Result of decoder_input sequencing: \n","<SOS> 하루가 또 가네요. <EOS> [1, 77, 78, 79, 2]\n","<SOS> 위로해 드립니다. <EOS> [1, 80, 81, 2]\n","<SOS> 여행은 언제나 좋죠. <EOS> [1, 13, 14, 15, 2]\n","\n","Result of decoder_target sequencing: \n","하루가 또 가네요. <EOS> [77, 78, 79, 2]\n","위로해 드립니다. <EOS> [80, 81, 2]\n","여행은 언제나 좋죠. <EOS> [13, 14, 15, 2]\n"]}]},{"cell_type":"code","source":["# 문장의 maxlen 설정하기\n","# source와 target 문장 모두에서의 최대 길이를 구한다\n","sentence_max_length = inputs_outputs.apply(lambda x: len(x.split())).max()\n","print('sentence max length: ', sentence_max_length)"],"metadata":{"id":"07f_kpUVhZJr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666659788583,"user_tz":-540,"elapsed":13,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"fcd39eb2-22f0-47f7-e4ef-c6b872e47945"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["sentence max length:  8\n"]}]},{"cell_type":"code","source":["# Data Padding\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","encoder_input_pad = pad_sequences(encoder_input, maxlen=sentence_max_length, padding='post')\n","decoder_input_pad = pad_sequences(decoder_input, maxlen=sentence_max_length, padding='post')\n","decoder_target_pad = pad_sequences(decoder_target, maxlen=sentence_max_length, padding='post')"],"metadata":{"id":"YQfZcT25hH8b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 타입 확인\n","print('\\nencoder_input_pad shape: ', encoder_input_pad.shape)\n","print(\"inputs: \", inputs[1])\n","print(\"encoder_input: \", encoder_input[1])\n","print(\"encoder_input_pad: \", encoder_input_pad[1])\n","\n","print('\\ndecoder_input_pad shape: ', decoder_input_pad.shape)\n","print(\"outputs_input: \", outputs_input[1])\n","print(\"decoder_input: \", decoder_input[1])\n","print(\"decoder_input_pad: \", decoder_input_pad[1])\n","\n","print('\\ndecoder_target_pad shape: ', decoder_target_pad.shape)\n","print(\"outputs_target: \", outputs_target[1])\n","print(\"decoder_target: \", decoder_target[1])\n","print(\"decoder_target_pad: \", decoder_target_pad[1])"],"metadata":{"id":"1SnD8pK5k6CO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666659788584,"user_tz":-540,"elapsed":13,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"fa5645d4-87bf-4d16-f53b-3e96827c0ad9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","encoder_input_pad shape:  (19, 8)\n","inputs:  1지망 학교 떨어졌어\n","encoder_input:  [34, 35, 36]\n","encoder_input_pad:  [34 35 36  0  0  0  0  0]\n","\n","decoder_input_pad shape:  (19, 8)\n","outputs_input:  <SOS> 위로해 드립니다. <EOS>\n","decoder_input:  [1, 80, 81, 2]\n","decoder_input_pad:  [ 1 80 81  2  0  0  0  0]\n","\n","decoder_target_pad shape:  (19, 8)\n","outputs_target:  위로해 드립니다. <EOS>\n","decoder_target:  [80, 81, 2]\n","decoder_target_pad:  [80 81  2  0  0  0  0  0]\n"]}]},{"cell_type":"markdown","source":["관련 라이브러리 임포트"],"metadata":{"id":"0gUW21qcBZTY"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import pandas as pd\n","\n","import enum\n","import os\n","import re\n","import json\n","\n","from sklearn.model_selection import train_test_split\n","from keras.callbacks import EarlyStopping, ModelCheckpoint\n","import matplotlib.pyplot as plt"],"metadata":{"id":"1w0niWbpl5Rz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 랜덤 시드 사용\n","# 실제에서는 이 부분을 제외한다\n","SEED_NUM = 1234\n","tf.random.set_seed(SEED_NUM)"],"metadata":{"id":"1--rFRkhmkyp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["문장 특수 기호 처리"],"metadata":{"id":"gYIPM-xkPsmZ"}},{"cell_type":"code","source":["PAD_INDEX = 0\n","STD_INDEX = 1\n","END_INDEX = 2\n","\n","# 변수명 변경\n","index_inputs = encoder_input_pad\n","index_outputs = decoder_input_pad\n","index_targets = decoder_target_pad\n","\n","# prepro_configs 설정\n","char2idx_dict = word_index\n","idx2char_dict = {y: x for x, y in word_index.items()}\n","\n","# dictionary 추가 및 변경\n","# dict_ex[new_key] = dict_ex[old_key]\n","# del dict_ex[old_key]\n","char2idx_dict['<PAD>'] = 0\n","\n","char2idx_dict['<SOS>'] = char2idx_dict['SOS']\n","del char2idx_dict['SOS']\n","\n","char2idx_dict['<END>'] = char2idx_dict['EOS']\n","del char2idx_dict['EOS']\n","\n","idx2char_dict[0] = '<PAD>'\n","idx2char_dict[1] = '<SOS>'\n","idx2char_dict[2] = '<END>'\n","\n","prepro_configs = dict({'char2idx':char2idx_dict, 'idx2char':idx2char_dict, 'vocab_size':len(word_index), 'pad_symbol': '<PAD>', 'std_symbol': '<SOS>', 'end_symbol': '<END>'})\n","print(prepro_configs)"],"metadata":{"id":"nyW8OiptmsqP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1666659788584,"user_tz":-540,"elapsed":11,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"20bfa152-f0df-4d24-fa06-1ec2c907bcae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'char2idx': {'SNS': 3, '다시': 4, '거예요': 5, '3박4일': 6, '놀러가고': 7, '싶다': 8, 'SD카드': 9, '가끔': 10, '궁금해': 11, '가스불': 12, '여행은': 13, '언제나': 14, '좋죠': 15, '새로': 16, '사는': 17, '게': 18, '마음': 19, '편해요': 20, '시간을': 21, '정하고': 22, '해보세요': 23, '그': 24, '사람도': 25, '그럴': 26, '빨리': 27, '집에': 28, '돌아가서': 29, '끄고': 30, '나오세요': 31, '12시': 32, '땡': 33, '1지망': 34, '학교': 35, '떨어졌어': 36, '정도': 37, 'PPL': 38, '심하네': 39, '망가졌어': 40, '안돼': 41, '맞팔': 42, '왜': 43, '안하지ㅠㅠ': 44, '시간낭비인': 45, '거': 46, '아는데': 47, '매일': 48, '하는': 49, '중': 50, '시간낭비인데': 51, '자꾸': 52, '보게됨': 53, 'SNS보면': 54, '나만': 55, '빼고': 56, '다': 57, '행복해보여': 58, '뭐하는지': 59, '가끔은': 60, '혼자인게': 61, '좋다': 62, '가난한': 63, '자의': 64, '설움': 65, '가만': 66, '있어도': 67, '땀난다': 68, '가상화폐': 69, '쫄딱': 70, '망함': 71, '켜고': 72, '나갔어': 73, '켜놓고': 74, '나온거': 75, '같아': 76, '하루가': 77, '또': 78, '가네요': 79, '위로해': 80, '드립니다': 81, '눈살이': 82, '찌푸려지죠': 83, '잘': 84, '모르고': 85, '있을': 86, '수도': 87, '있어요': 88, '자랑하는': 89, '자리니까요': 90, '혼자를': 91, '즐기세요': 92, '돈은': 93, '들어올': 94, '땀을': 95, '식혀주세요': 96, '어서': 97, '잊고': 98, '새출발': 99, '하세요': 100, '<PAD>': 0, '<SOS>': 1, '<END>': 2}, 'idx2char': {1: '<SOS>', 2: '<END>', 3: 'SNS', 4: '다시', 5: '거예요', 6: '3박4일', 7: '놀러가고', 8: '싶다', 9: 'SD카드', 10: '가끔', 11: '궁금해', 12: '가스불', 13: '여행은', 14: '언제나', 15: '좋죠', 16: '새로', 17: '사는', 18: '게', 19: '마음', 20: '편해요', 21: '시간을', 22: '정하고', 23: '해보세요', 24: '그', 25: '사람도', 26: '그럴', 27: '빨리', 28: '집에', 29: '돌아가서', 30: '끄고', 31: '나오세요', 32: '12시', 33: '땡', 34: '1지망', 35: '학교', 36: '떨어졌어', 37: '정도', 38: 'PPL', 39: '심하네', 40: '망가졌어', 41: '안돼', 42: '맞팔', 43: '왜', 44: '안하지ㅠㅠ', 45: '시간낭비인', 46: '거', 47: '아는데', 48: '매일', 49: '하는', 50: '중', 51: '시간낭비인데', 52: '자꾸', 53: '보게됨', 54: 'SNS보면', 55: '나만', 56: '빼고', 57: '다', 58: '행복해보여', 59: '뭐하는지', 60: '가끔은', 61: '혼자인게', 62: '좋다', 63: '가난한', 64: '자의', 65: '설움', 66: '가만', 67: '있어도', 68: '땀난다', 69: '가상화폐', 70: '쫄딱', 71: '망함', 72: '켜고', 73: '나갔어', 74: '켜놓고', 75: '나온거', 76: '같아', 77: '하루가', 78: '또', 79: '가네요', 80: '위로해', 81: '드립니다', 82: '눈살이', 83: '찌푸려지죠', 84: '잘', 85: '모르고', 86: '있을', 87: '수도', 88: '있어요', 89: '자랑하는', 90: '자리니까요', 91: '혼자를', 92: '즐기세요', 93: '돈은', 94: '들어올', 95: '땀을', 96: '식혀주세요', 97: '어서', 98: '잊고', 99: '새출발', 100: '하세요', 0: '<PAD>'}, 'vocab_size': 101, 'pad_symbol': '<PAD>', 'std_symbol': '<SOS>', 'end_symbol': '<END>'}\n"]}]},{"cell_type":"markdown","source":["모델 하이퍼파라미터 정의"],"metadata":{"id":"6Nli342DPQGt"}},{"cell_type":"code","source":["char2idx = prepro_configs['char2idx']               # {'SNS': 3, '다시': 4, ..., , '<PAD>': 0, '<SOS>': 1, '<END>': 2}}의 딕셔너리\n","end_index = prepro_configs['end_symbol']            # end_index == '<END>'\n","vocab_size = prepro_configs['vocab_size']\n","BATCH_SIZE = 2\n","MAX_SEQUENCE = 25                                   # 최대 시퀀스 길이\n","EPOCHS = 30\n","VALID_SPLIT = 0.1\n","\n","kargs = {'model_name': model_name,\n","         'num_layers': 2,                           # 사용할 인코더 레이어의 개수\n","         'd_model': 512,                            # 임베딩 차원(dimension_model): query, key, value에 대한 임베딩 차원\n","         'num_heads': 8,                            # 어텐션 헤드 수\n","         'dff': 2048,                               # dimension of Feed Forward Network. 피드 포워드 네트워크 층의 노드 수\n","         'input_vocab_size': vocab_size,            # 단어 사전의 수\n","         'target_vocab_size': vocab_size,           # 단어 사전의 수\n","         'maximum_position_encoding': MAX_SEQUENCE, # 포지션 인코더의 최대 시퀀스 길이\n","         'end_token_idx': char2idx[end_index],      # char2idx[end_index] == 2. 종료 표지의 인덱스\n","         'rate': 0.1                                # Dropout에 사용되는 비율\n","        }"],"metadata":{"id":"n5lw_MHuyCZD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["순방향 마스크"],"metadata":{"id":"mbpLubr-Ojmg"}},{"cell_type":"code","source":["# seq의 값이 padding 0일 때만 1.0을 출력하고, 그 외에는 0을 출력하는 함수\n","# 마스킹 대상을 1.0으로 만든다. 이후 -1e9라는 작은 수를 곱하고,\n","# 후에 softmax() 함수를 거치면서 값이 역전된다\n","# 입력(batch_size, seq_len) --> return(batch_size, 1, 1, seq_len)로 차원 늘림\n","# 이후 attention의 (batch_size, heads, en/decoder_len, seq_len)에 합산됨\n","def create_padding_mask(seq):\n","    mask = tf.cast(tf.math.equal(seq, 0), tf.float32)                                                   \n","                                                      \n","    return mask[:, tf.newaxis, tf.newaxis, :]         "],"metadata":{"id":"cEErD4lByIE8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 우삼각부분만 1로 마스킹 영역을 표시한다\n","def create_look_ahead_mask(size):\n","    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n","    return mask                                      # (seq_len, seq_len)"],"metadata":{"id":"V6_FseFLyNtn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def create_masks(inp, tar):\n","    enc_padding_mask = create_padding_mask(inp)      # 인코더 패딩 마스크\n","    dec_padding_mask = create_padding_mask(inp)      # 디코더 두 번째 어텐션 블록에서 사용되는 패딩 마스크\n","\n","    #print(\"tf.shape(tar):\", tf.shape(tar))\n","    #print(\"tf.shape(tar)[1]:\", tf.shape(tar)[1])\n","    #print(\"tar:\\n\", tar)\n","\n","    # 디코더의 첫 번째 어텐션 블록에서 사용되는 마스크\n","    # 디코더가 받은 데이터를 패딩 처리 이후 순방향 마스킹을 하여 미래의 단어가 참고되지 않게 한다\n","    # combined_mask는 look_ahead_mask 라는 이름으로 사용될 것이다\n","    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n","    dec_target_padding_mask = create_padding_mask(tar)\n","    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n","\n","    return enc_padding_mask, combined_mask, dec_padding_mask\n","\n","\n","# 마스크 실행\n","enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(index_inputs, index_outputs)"],"metadata":{"id":"JQ6brLAlyP7N"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["enc_padding_mask"],"metadata":{"id":"EF9wnwP9oCBL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["look_ahead_mask"],"metadata":{"id":"YT-Z9kHooDQ7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dec_padding_mask"],"metadata":{"id":"uPdxwQQhoEBb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["포지셔널 인코딩"],"metadata":{"id":"EFNrISvtyb6N"}},{"cell_type":"code","source":["def get_angles(pos, i, d_model):\n","  angle_rates = 1 / np.power(10000, (2*i//2)/np.float32(d_model))\n","  return pos * angle_rates"],"metadata":{"id":"OA5Z6ekP5qtN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def positional_encoding(position, d_model):\n","  angle_rads = get_angles(np.arange(position)[:, np.newaxis], \n","                          np.arange(d_model)[np.newaxis, :],\n","                          d_model)\n","  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n","  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n","  pos_encoding = angle_rads[np.newaxis, ...]\n","\n","  return tf.cast(pos_encoding, dtype=tf.floast32)"],"metadata":{"id":"_cSDlngUz-L3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def scaled_dot_product_attention(q, k, v, mask):\n","  matmul_qk = tf.matmul(q, k, transpost_b=True)\n","\n","  dk = tf.cast(tf.shape(k)[-1], tf.float32)\n","  scaled_attention_logits = matmul_qk / tf.math.sqrk(dk)\n","\n","  if mask is not None:\n","    scaled_attention_logits += (mask * -1e9)\n","\n","  attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n","\n","  output = tf.matmul(attention_weights, v)\n","\n","  return output, attention_weights"],"metadata":{"id":"R_7dxpfv1VUB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiHeadAttention(tf.keras.layers.Layer):\n","  def __init__(self, **kargs):\n","    super(MultiHeadAttention, self).__init__()\n","    self.num_heads = kargs['num_heads']\n","    self.d_model = kargs['d_model']\n","\n","    assert self.d_model % self.num_heads == 0\n","\n","    self.depth = self.d_model // self.num_heads\n","\n","    # query, key, value 가중치 레이어 생성\n","    self.wq = tf.keras.layers.Dense(kargs['d_model'])\n","    self.wk = tf.keras.layers.Dense(kargs['d_model'])\n","    self.wv = tf.keras.layers.Dense(kargs['d_model'])\n","\n","    self.dense = tf.keras.layers.Dense(kargs['d_model'])\n","  \n","\n","  def split_heads(self, x, batch_size):\n","    x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n","\n","    return tf.transpose(x, perm=[0, 2, 1, 3])\n","\n","  \n","  def call(self, v, k, q, mask):\n","    batch_size = tf.shape(q)[0]\n","\n","    q = self.wq(q)\n","    k = self.wk(k)\n","    v = self.wv(v)\n","\n","    q = self.split_heads(q, batch_size)\n","    k = self.split_heads(k, batch_size)\n","    v = self.split_heads(v, batch_size)\n","\n","    scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n","    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n","    concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n","\n","    output = self.dense(concat_attention)\n","\n","    return output, attention_weights\n"],"metadata":{"id":"VTLYVPweJqkj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def feed_forward_network(**kargs):\n","  return tf.keras.Sequential([\n","      tf.keras.layers.Dense(kargs['dff'], activation='relu'),\n","      tf.keras.layers.Dense(kargs['d_model'])\n","  ])"],"metadata":{"id":"TdCE2hdmQJP4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class EncoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, **kargs):\n","    super(EncoderLayer, self).__init__()\n","\n","    self.mha = MultiHeadAttention(**kargs)\n","    self.ffn = feed_forward_network(**kargs)\n","\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.dropout1 = tf.keras.layers.Dropout(kargs['rate'])\n","    self.dropout2 = tf.keras.layers.Dropout(kargs['rate'])\n","\n","  def call(self, x, mask):\n","    attn_output, _ = self.mha(x, x, x, mask)\n","    attn_output = self.dropout1(attn_output)\n","    out1 = self.layernorm1(x + attn_output)\n","\n","    ffn_output = self.ffn(out1)\n","    ffn_output = self.dropout2(ffn_output)\n","    out2 = self.layernorm2(out1 + ffn_output)\n","\n","    return out2"],"metadata":{"id":"p5my_tc4sPHr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = np.array([0.006, 0.002, 0.004, 0.009, 0.005, 0.076, 0.007, 0.008, 0.003])\n","data = data.reshape(-1, 3)\n","print(\"data\", data)\n","\n","layer = tf.keras.layers.LayerNormalization(axis=1)\n","output = layer(data)\n","print(\"output\", output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KpaxkRw3tYmi","executionInfo":{"status":"ok","timestamp":1666677461273,"user_tz":-540,"elapsed":418,"user":{"displayName":"최석재","userId":"09969236112114002539"}},"outputId":"822e54e9-718e-427c-87f2-215e38d830c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["data [[0.006 0.002 0.004]\n"," [0.009 0.005 0.076]\n"," [0.007 0.008 0.003]]\n","output tf.Tensor(\n","[[ 0.06316139 -0.0631614   0.        ]\n"," [-0.46261042 -0.55072665  1.013337  ]\n"," [ 0.03154924  0.06309849 -0.09464776]], shape=(3, 3), dtype=float32)\n"]}]},{"cell_type":"code","source":["class DecoderLayer(tf.keras.layers.Layer):\n","  def __init__(self, **kargs):\n","    super(DecoderLayer, self).__init__()\n","\n","    self.mha1 = MultiHeadAttention(**kargs)\n","    self.mha2 = MultiHeadAttention(**kargs)\n","    self.ffn = feed_forward_network(**kargs)\n","\n","    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n","\n","    self.dropout1 = tf.keras.Dropout(kargs['rate'])\n","    self.dropout2 = tf.keras.Dropout(kargs['rate'])\n","    self.dropout3 = tf.keras.Dropout(kargs['rate'])\n","\n","  \n","  def call(self, x, enc_output, look_ahead_mask, padding_mask):\n","    attn1, attn_weights_block1 = self,mha1(x, x, x, look_ahead_mask)\n","    attn1 = self.dropout1(attn1)\n","    out1 = self.layernorm1(attn1 + x)\n","\n","    attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n","    attn2 = self.dropout2(attn2)\n","    out2 = self.layernorm2(attn2 + out1)\n","\n","    ffn_output = self.ffn(out2)\n","    ffn_output = self.dropout3(ffn_output)\n","    out3 = self.layernorm3(ffn_output + out2)\n","\n","    return out3, attn_weights_block1, attn_weights_block2"],"metadata":{"id":"DYsBD6lRtw9I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Encoder(tf.keras.layers.Layer):\n","  def __init__(self, **kargs):\n","    super(Encoder, self).__init__()\n","\n","    self.d_model = kargs['d_model']\n","    self.num_layers = kargs['num_layers']\n","\n","    # 워드 임베딩 레이어 생성\n","    self.embedding = tf.keras.layers.Embedding(input_dim=kargs['input_vocab_size'], output_dim=self.d_model)\n","    self.pos_encoding = positional_encoding(position=kargs['maximum_position_encoding'], d_model=self.d_model)\n","    self.enc_layers = [EncoderLayer(**kargs) for _ in range(self.num_layers)]\n","    self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n","\n","  def call(self, x, mask)    :\n","    seq_len = tf.shape(x)[1]\n","\n","    x = self.embewdding(x)\n","    x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n","    x += self.pos_encoding[:, :seq_len, :]\n","\n","    x = self.dropout(x)\n","\n","    for i in range(self.num_layers):\n","      x = self.enc_layers[i](x, mask)\n","    \n","    return x"],"metadata":{"id":"SPIJC1bkBkUx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(tf.keras.layers.Layer):\n","  def __init__(self, **kargs):\n","    super(Decoder, self).__init__()\n","\n","    self.d_model = kargs['d_model']\n","    self.num_layers = kargs['num_layers']\n","\n","    self.embedding = tf.keras.layers.Embedding(input_dim=kargs['target_vocab_size'], output_dim=self.d_model)\n","    self.pos_encoding = positional_encoding(position=kargs['maximum_position_encoding'], d_model=self.d_model)\n","    self.dec_layers = [DecoderLayer(**kargs) for _ in range(self.num_layers)]\n","\n","    self.dropout = tf.keras.layers.Dropout(kargs['rate'])\n","\n","    for i in range(self.num_layers):\n","      x, block1, block2 = self.dec_layers[i](x, enc_output, look_ahead_mask, padding_mask)\n","      attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n","      attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n","    \n","    return x, attention_weights"],"metadata":{"id":"PWne0MWBGhGU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Transformer(tf.keras.Model):\n","  def __init__(self, **kargs):\n","    super(Transformer, self).__init__(name=kargs['model_name'])\n","    self.end_token_idx = kargs['end_token_idx']\n","\n","    self.encoder = Encoder(**kargs)\n","    self.decoder = Decoder(**kargs)\n","\n","    self.final_layer = tf.keras.layers.Dense(kargs['target_vocab_size'])\n","\n","  def call(self, x):\n","    inp, tar = x\n","\n","    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n","    enc_output = self.encoder(inp, enc_padding_mask)\n","    dec_output, _ = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n","    final_output = self.final_layer(dec_output)\n","\n","    return final_output\n","\n","  def inference(self, x):\n","    inp = x\n","    tar = tf.expand_dims([STD_INDEX], axis=0)\n","    enc_padding_mask, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n","    enc_output = self.encoder(inp, enc_padding_mask)\n","\n","    predict_tokens = list()\n","    for t in range(0, MAX_SEQUENCE):\n","      dec_output, _ = self.decoder(tar, enc_output, look_ahead_mask, dec_padding_mask)\n","      final_output = self.final_layer(dec_output)\n","      outputs = tf.argmax(final_output, axis=-1).numpy()\n","      print(\"outputs:\", outputs)\n","      pred_token = outputs[0][-1]\n","      if pred_token == self.end_token_idx:\n","        break\n","      predict_tokens.append(pred_token)\n","      tar = tf.expand_dims([STD_INDEX] + predict_tokens, axis=0)\n","      _, look_ahead_mask, dec_padding_mask = create_masks(inp, tar)\n","    \n","    return predict_tokens"],"metadata":{"id":"2vpsOzbdKE46"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"D_k7nAMOPN7p"},"execution_count":null,"outputs":[]}]}